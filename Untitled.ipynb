{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "super-holocaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 25 00:42:41 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:3B:00.0 Off |                  Off |\r\n",
      "| 33%   24C    P8    17W / 260W |  24376MiB / 48601MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Quadro RTX 8000     Off  | 00000000:86:00.0 Off |                  Off |\r\n",
      "| 32%   32C    P0    32W / 260W |      0MiB / 48601MiB |      5%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Quadro RTX 8000     Off  | 00000000:AF:00.0 Off |                  Off |\r\n",
      "| 33%   25C    P8     6W / 260W |   1378MiB / 48601MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "underlying-rolling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobart\n",
      "  Cloning https://github.com/SKT-AI/KoBART to /tmp/pip-install-mlode_11/kobart\n",
      "  Running command git clone -q https://github.com/SKT-AI/KoBART /tmp/pip-install-mlode_11/kobart\n",
      "Collecting transformers==4.3.3\n",
      "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 440 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.7.1\n",
      "  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 776.8 MB 5.5 kB/s eta 0:00:013    |███████▎                        | 177.9 MB 8.0 MB/s eta 0:01:16     |███████▋                        | 183.5 MB 4.9 MB/s eta 0:02:01     |████████▎                       | 200.9 MB 8.0 MB/s eta 0:01:12     |██████████▏                     | 247.9 MB 5.7 MB/s eta 0:01:34     |██████████▎                     | 249.5 MB 5.7 MB/s eta 0:01:33     |██████████████████▎             | 444.3 MB 8.1 MB/s eta 0:00:42     |██████████████████▍             | 446.6 MB 5.4 MB/s eta 0:01:02     |███████████████████▊            | 478.1 MB 10.8 MB/s eta 0:00:28     |████████████████████            | 483.9 MB 7.3 MB/s eta 0:00:40     |█████████████████████▉          | 529.6 MB 6.0 MB/s eta 0:00:42     |██████████████████████▎         | 540.0 MB 83.5 MB/s eta 0:00:03     |███████████████████████         | 560.8 MB 7.1 MB/s eta 0:00:31     |███████████████████████▏        | 563.7 MB 8.0 MB/s eta 0:00:27     |████████████████████████▋       | 596.2 MB 5.0 MB/s eta 0:00:37     |████████████████████████▋       | 597.6 MB 5.0 MB/s eta 0:00:36     |██████████████████████████      | 631.6 MB 5.2 MB/s eta 0:00:28     |██████████████████████████▊     | 648.6 MB 4.5 MB/s eta 0:00:29\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (2020.11.13)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.1-cp38-cp38-manylinux2010_x86_64.whl (3.2 MB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (1.19.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (4.46.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (0.0.43)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.7.1->kobart) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.3.3->kobart) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.3.3->kobart) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.3.3->kobart) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.3.3->kobart) (2.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers==4.3.3->kobart) (2.4.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.3.3->kobart) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.3.3->kobart) (1.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.3.3->kobart) (7.1.2)\n",
      "Building wheels for collected packages: kobart\n",
      "  Building wheel for kobart (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobart: filename=kobart-0.4-py3-none-any.whl size=8504 sha256=2b552c7b8434a305f1e09c33acd11f90f46eafbd59b9d8556e1c5d1dd6a2d007\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hadvu6kq/wheels/08/af/7d/32d2d2d08c8009730f02412f3d62a364a2c1bf477803f9183f\n",
      "Successfully built kobart\n",
      "Installing collected packages: tokenizers, transformers, torch, kobart\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.4\n",
      "    Uninstalling tokenizers-0.9.4:\n",
      "      Successfully uninstalled tokenizers-0.9.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.2.0\n",
      "    Uninstalling transformers-4.2.0:\n",
      "      Successfully uninstalled transformers-4.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.7.0\n",
      "    Uninstalling torch-1.7.0:\n",
      "      Successfully uninstalled torch-1.7.0\n",
      "Successfully installed kobart-0.4 tokenizers-0.10.1 torch-1.7.1 transformers-4.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ranging-chase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KoBART-chatbot'...\n",
      "remote: Enumerating objects: 34, done.\u001b[K\n",
      "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 34 (delta 14), reused 24 (delta 8), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (34/34), done.\n",
      "Submodule 'Chatbot_data' (https://github.com/haven-jeon/Chatbot_data.git) registered for path 'Chatbot_data'\n",
      "Cloning into '/workspace/koBART/KoBART-chatbot/Chatbot_data'...\n",
      "remote: Enumerating objects: 4, done.        \n",
      "remote: Counting objects: 100% (4/4), done.        \n",
      "remote: Compressing objects: 100% (4/4), done.        \n",
      "remote: Total 24 (delta 0), reused 3 (delta 0), pack-reused 20        \n",
      "Submodule path 'Chatbot_data': checked out '05e9a64f49a1d0f040ffc63d74b132e8b1795a23'\n"
     ]
    }
   ],
   "source": [
    "!git clone --recurse-submodules https://github.com/haven-jeon/KoBART-chatbot.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "amazing-gazette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/koBART/KoBART-chatbot\n"
     ]
    }
   ],
   "source": [
    "%cd KoBART-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ultimate-highway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n",
      "[██████████████████████████████████████████████████]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./kobart_from_pretrained'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kobart import get_pytorch_kobart_model, get_kobart_tokenizer\n",
    "get_kobart_tokenizer(\".\")\n",
    "get_pytorch_kobart_model(cachedir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "!python kobart_chit_chat.py  --gradient_clip_val 1.0 --max_epochs 3 --default_root_dir logs --model_path kobart_from_pretrained  --tokenizer_path emji_tokenizer --chat --gpus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-representative",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
